<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <title>RegNeRF</title>
</head>

<body>
  <div class="container">
    <br>
    <div style="text-align: center;">
      <h1>RegNeRF</h1>
      <h3> Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</h3>
      <div style="margin-top: 15px;">
        <span style="margin-right: 15px; font-size: 1.3em;">Michael Niemeyer<sup>1,2,3</sup></span>
        <span style="margin-right: 15px; font-size: 1.3em;">Jonathan T. Barron<sup>3</sup></span>
        <span style="margin-right: 15px; font-size: 1.3em;">Ben Mildenhall<sup>3</sup></span>
        </div>
        <div>
        <span style="margin-right: 15px; font-size: 1.3em;">Mehdi S. M. Sajjadi<sup>3</sup></span>
        <span style="margin-right: 15px; font-size: 1.3em;">Andreas Geiger<sup>1,2</sup></span>
        <span style="font-size: 1.3em;">Noha Radwan<sup>3</sup></span>
      </div>
      <div style="margin-top: 15px;">
        <span style="margin-right: 20px; font-size: 1.2em;"><sup>1</sup>Max Planck Institute for Intelligent Systems</span> 
        <span style="margin-right: 20px; font-size: 1.2em;"><sup>2</sup>University of TÃ¼bingen</span>
        <span style="font-size: 1.2em;"><sup>3</sup>Google Research</span>
      </div>
      <div>
        <span style="margin-right: 10px; font-size: 1.3em;">arXiv 2021</span>
      </div>
    </div>
    <div class="row">
      <div class="col-md-12 col-sm-12 col-xs-12">
        <div class="embed-responsive embed-responsive-21by9">
          <video controls loop muted autoplay class="embed-responsive-item">
            <source src="gfx/room_videos/n3_text.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <div class="text-center" style="font-size: 1.5em;">
      <a href="http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf" target="_blank" style="margin-right: 20px;">[Paper]</a>
      <!-- <a href="http://www.cvlibs.net/publications/Niemeyer2021CVPR_supplementary.pdf" target="_blank" style="margin-right: 20px;">[Supplementary]</a>
      <a href="https://github.com/autonomousvision/giraffe" target="_blank" style="margin-right: 20px;">[Code]</a>
      <a href="http://autonomousvision.github.io/giraffe" target="_blank" style="margin-right: 20px;">[Blog]</a>
      <a href="http://www.youtube.com/watch?v=fIaDXC-qRSg&vq=hd1080&autoplay=1" target="_blank" style="margin-right: 20px;">[Video]</a>
      <a href="https://m-niemeyer.github.io/slides/talks/giraffe/index.html" target="_blank" style="margin-right: 20px;">[Interactive Slides]</a> -->
      <a href="https://www.youtube.com/watch?v=scnXyCSMJF4" target="_blank">[Video]</a>
    </div>
    <div style="margin-top: 30px;">
      <h2 class="text-center">
        Abstract
      </h2>
      <p style="font-style: italic;">
        Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.
      </p>
      <p style="font-size: 1.2em;">
        <span style="font-weight: bold;">TL;DR:</span> We regularize unseen views during optimization to enable view synthesis from sparse inputs with as few as 3 input images.
      </p>
    </div>
    <div style="margin-top:10px;">
      <h2 class="text-center">
        Video
      </h2>
      <div class="embed-responsive embed-responsive-16by9">
        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/fIaDXC-qRSg" allowfullscreen></iframe>
      </div>
    </div>
    <div style="margin-top: 50px;">
      <div class="text-center">
        <h2>
          Method Overview
        </h2>
        <img src="gfx/teaser.svg" width=100% class="img-fluid" alt="Responsive image">
      </div>
      <div style="margin-top: 35px;">
        <p>
          NeRF optimizes the reconstruction loss for a given set of input images (<span class="text-primary">blue cameras</span>).
          For sparse inputs, however, this leads to degenerate solutions.
          In this work, we propose to sample unobserved views (<span class="text-danger">red cameras</span>) and <span style="font-weight: bold;"> regularize the geometry and appearance of patches</span> rendered from those views.
          More specifically, we cast rays through the scene and render patches from unobserved viewpoints for a given radiance field f.
          We then regularize appearance by feeding the predicted RGB patches through a trained normalizing flow model phi
          and maximizing predicted log-likelihood.
          We regularize geometry by enforcing a smoothness loss on the rendered depth patches.
          Our approach leads to 3D-consistent representations <span style="font-weight: bold;">even for sparse input scenarios with as few as 3 input views</span> from which realistic novel views can be rendered.
        </p>
      </div>    
    </div>

    <div style="margin-top:50px;">
      <h2 class="text-center">
        Results
      </h2>
      <h4>View Synthesis from 3 Input Views</h4>
      <p>
        While mip-NeRF leads to degenerate view synthesis and predicted scene geometry, our method enables realistic view synthesis from 3 input views.
        </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="gfx/room_videos/n3_text.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h4>View Synthesis from 6 Input Views</h4>
      <p>
        For 6 input views, mip-NeRF improves but predicted renderings and the optimized scene geometry still contain floating artifacts. Our approach leads to smooth predicted scene geometry and realistic novel views.
        </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="gfx/room_videos/n6_text.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h4>View Synthesis from 9 Input Views</h4>
      <p>
        For 9 input views, mip-NeRF and our method both lead to high-quality view synthesis. For mip-NeRF, small floating artfiacts for far-away novel views near the table are still visible while our predicted scene geometry appears more realistic.
      </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="gfx/room_videos/n9_text.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <p>
        For more results, see <a href="comparisons.html" target="_blank">the comparisons page</a>.
      </p>
    <div>
      <h2 class="text-center">
        Citation
      </h2>
      <p>
        If you want to cite our work, please use:
      </p>
      <pre>
        @article{Niemeyer2021Regnerf,
          author    = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},  
          title     = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
          journal   = {arXiv.org},
          year      = {2021},
        }
      </pre>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
      integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
      crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
      integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
      crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
      integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
      crossorigin="anonymous"></script>
</body>

</html>
